# LORAID System: Deterministic Parameter Space Allocation for LoRA

> **‚ö†Ô∏è Model Compatibility**: This LORAID implementation has been tested exclusively on **Wan 2.2 (14B parameter DiT model)**. It may require modifications for other models like Hunyuan, as block counts and architecture patterns could differ.

Note: Code was made written by Claude Code with a whip. Still testing some stuff, but training, inferencing and merging works. The basic idea was for this, to be able to train a big dataset in slices. If I would merge my loras together or back to the DiT they would overwrite their knowledge. I wanted a way to prevent that. You can think of this as a nice experimental project :) With love!



## üéØ **Why LORAID?**

### **Problem: Catastrophic Forgetting with Traditional LoRA Merging**
When training 200+ LoRAs and merging them sequentially into a DiT model, traditional approaches suffer from:
- **Catastrophic Forgetting**: Later LoRAs overwrite knowledge from earlier LoRAs
- **Parameter Conflicts**: Multiple LoRAs compete for the same parameter space
- **Knowledge Loss**: Previously learned character identities get degraded over time
- **Unpredictable Results**: No guarantee which LoRA's knowledge will survive the merging process

### **Solution: Deterministic Parameter Space Allocation**
LORAID (LoRA ID) system provides:
- ‚úÖ **Zero Parameter Conflicts**: Each LoRA targets non-overlapping model parameters
- ‚úÖ **Preserved Knowledge**: All LoRAs retain their learned features indefinitely
- ‚úÖ **Scalable Training**: Support for 240+ independent LoRAs
- ‚úÖ **Predictable Behavior**: Deterministic parameter allocation based on LORAID
- ‚úÖ **Composable Effects**: Multiple LoRAs can work together without interference
- ‚úÖ **Hardware Efficiency**: Train large datasets in manageable slices

---

## üèóÔ∏è **Architecture Overview**

### **Parameter Space Allocation**
The DiT model has **240 transformer blocks** (blocks 0-239). LORAID allocates these blocks deterministically:

```
Model: 240 blocks total (blocks.0 through blocks.239)
‚îú‚îÄ‚îÄ LORAID 1:  blocks 0-11    (12 blocks)
‚îú‚îÄ‚îÄ LORAID 2:  blocks 12-23   (12 blocks)  
‚îú‚îÄ‚îÄ LORAID 3:  blocks 24-35   (12 blocks)
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ LORAID 20: blocks 228-239 (12 blocks)
```

**Scalability Options:**
- **12 blocks per LORAID**: 20 LORAIDs maximum (current default)
- **3 blocks per LORAID**: 80 LORAIDs maximum  
- **2 blocks per LORAID**: 120 LORAIDs maximum
- **1 block per LORAID**: 240 LORAIDs maximum

Each block contains ~10 weight matrices (self_attn, cross_attn, ffn) with millions of parameters per LORAID.

### **How It Works**
1. **Training**: Each LORAID targets only its designated blocks using include/exclude patterns
2. **Inference**: Multiple LORAIDs can be loaded simultaneously without conflicts
3. **Merging**: LoRAs merge into their designated parameter spaces only
4. **Composition**: Neural network processes through multiple modified block ranges

---

## üìÅ **Main Modified Files**

### **Core Training Files**
- `src/musubi_tuner/wan_train_network.py` - LORAID argument parsing and parameter allocation
- `src/musubi_tuner/networks/lora_wan.py` - LoRA network creation with LORAID filtering
- `src/musubi_tuner/networks/lora.py` - Include/exclude pattern logic and metadata storage

### **Inference Files**  
- `src/musubi_tuner/wan_generate_video.py` - Multi-LORAID inference and merging
- `src/musubi_tuner/utils/lora_utils.py` - LORAID compatibility validation and loading

### **Post-Processing Files**
- `src/musubi_tuner/lora_post_hoc_ema.py` - LORAID-aware EMA and democratic blending - EXPERIMENTAL -

---

## üõ†Ô∏è **Implementation Details**

### **1. Parameter Pattern Generation**
```python
def get_loraid_parameter_patterns(loraid):
    """Generate include patterns for LORAID parameter allocation"""
    total_blocks = 240
    blocks_per_loraid = 12  # Configurable
    start_block = (loraid - 1) * blocks_per_loraid  
    end_block = min(start_block + blocks_per_loraid - 1, total_blocks - 1)
    
    include_patterns = []
    for block_id in range(start_block, end_block + 1):
        patterns = [
            f"blocks\\.{block_id}\\.self_attn\\.",
            f"blocks\\.{block_id}\\.cross_attn\\.",  
            f"blocks\\.{block_id}\\.ffn\\."
        ]
        include_patterns.extend(patterns)
    
    return include_patterns
```

### **2. Whitelist/Blacklist Logic** 
```python
# networks/lora.py - Fixed include/exclude precedence
if include_re_patterns:
    # 1. Inclusion check: Only modules in whitelist pass
    included = any(pattern.match(original_name) for pattern in include_re_patterns)
    if not included:
        continue

# 2. Exclusion check: Blacklist overrides whitelist  
excluded = any(pattern.match(original_name) for pattern in exclude_re_patterns)
if excluded:
    continue
```

### **3. LORAID Metadata Storage**
```python
# networks/lora.py - save_weights()
if hasattr(self, '_loraid_value') and self._loraid_value is not None:
    metadata["ss_loraid"] = str(self._loraid_value)
    logger.info(f"Added LORAID {self._loraid_value} to saved model metadata")
```

### **4. Multi-LORAID Argument Parsing**
```python
# wan_generate_video.py
parser.add_argument(
    "--LORAID", type=int, nargs="*", action="append", default=None,
    help="Supports '--LORAID 1 2 3' or '--LORAID 1 --LORAID 2 --LORAID 3'"
)

def parse_loraid_args(args):
    target_loraids = []
    for loraid_list in args.LORAID:
        if isinstance(loraid_list, list):
            target_loraids.extend(loraid_list)
        else:
            target_loraids.append(loraid_list)
    return list(dict.fromkeys(target_loraids))  # Remove duplicates
```

---

## üìö **Usage Examples**

### **Training LORAIDs**
```bash
# Train LORAID 1 (targets blocks 0-11)
accelerate launch wan_train_network.py \
    --task t2v-A14B \
    --dit F:/models/wan22_t2v_14B.safetensors \
    --dataset_config F:/data/testset1_dataset.toml \
    --network_module networks.lora_wan \
    --network_dim 64 --network_alpha 64 \
    --output_name Sarah_512 \
    --LORAID 1

# Train LORAID 2 (targets blocks 12-23)  
accelerate launch wan_train_network.py \
    --task t2v-A14B \
    --dit F:/models/wan22_t2v_14B.safetensors \
    --dataset_config F:/data/testset2_dataset.toml \
    --network_module networks.lora_wan \
    --network_dim 64 --network_alpha 64 \
    --output_name Jessica_512 \
    --LORAID 2
```

### **Single LORAID Inference**
```bash
# Inference with only LORAID 1
python wan_generate_video.py \
    --task t2v-14B \
    --prompt "Sarah on a beach, standing, perfect" \
    --dit F:/models/wan22_t2v_14B.safetensors \
    --vae F:/models/wan_2.1_vae.safetensors \
    --t5 F:/models/t5-xxl.pth \
    --lora_weight F:/loras/Sarah_512-000010.safetensors \
    --lora_multiplier 1.0 \
    --LORAID 1 \
    --save_path F:/output/
```

### **Multi-LORAID Inference**
```bash
# Inference with multiple LORAIDs
python wan_generate_video.py \
    --task t2v-14B \
    --prompt "Sarah and jessica on a beach together" \
    --dit F:/models/wan22_t2v_14B.safetensors \
    --vae F:/models/wan_2.1_vae.safetensors \
    --t5 F:/models/t5-xxl.pth \
    --lora_weight F:/loras/Jessica_512-000010.safetensors,F:/loras/Sarah_512-000005.safetensors \
    --lora_multiplier 1.0 0.7 \
    --LORAID 1 2 \
    --save_path F:/output/
```

### **LoRA Multiplier Control**
```bash
# Fine-grained control over LoRA strength
# Only LORAID 1 active (Sarah only)
--LORAID 1 2 --lora_multiplier 1.0 0.0

# Only LORAID 2 active (Jessica only)  
--LORAID 1 2 --lora_multiplier 0.0 1.0

# Both at half strength
--LORAID 1 2 --lora_multiplier 0.5 0.5

# Mixed strengths
--LORAID 1 2 3 --lora_multiplier 1.0 0.7 0.3

# Lazy usage (defaults to 1.0 for all)
--LORAID 1 2 3 --lora_multiplier 1.0  # [1.0, 1.0, 1.0]
```

### **Model Merging**
```bash
# Merge multiple LORAIDs permanently into DiT
python wan_generate_video.py \
    --task t2v-14B \
    --dit F:/models/wan22_t2v_14B.safetensors \
    --lora_weight F:/loras/Sarah.safetensors,F:/loras/Jessica.safetensors \
    --lora_multiplier 1.0 1.0 \
    --LORAID 1 2 \
    --save_merged_model F:/models/merged.safetensors
```

### **Post-Hoc EMA with LORAID Preservation**
```bash
# EMA merge while preserving LORAID metadata, script automatically detects sibling and subfolder epochs
python lora_post_hoc_ema.py \
    --sigma_rel 0.2 \
    --lora1 F:/loras/Sarah_epoch_005.safetensors \
    --lora2 F:/loras/Sarah_epoch_010.safetensors \
    --output_file F:/loras/Sarah_ema.safetensors \
    --average 1.0 # Regular lora blendy merge

# Democratic knowledge sharing between LORAIDs, EXPERIMENTAL
python lora_post_hoc_ema.py \
    --sigma_rel 0.2 \
    --lora1 F:/loras/Sarah.safetensors \
    --lora2 F:/loras/jessica.safetensors \
    --output_file_lora1 F:/loras/Sarah_shared.safetensors \
    --output_file_lora2 F:/loras/jessica_shared.safetensors \
    --average 1.0 --democratic
```

---

## ‚öôÔ∏è **Configuration Options**

### **Block Allocation Strategies**
Modify `blocks_per_loraid` in `wan_train_network.py`:

```python
# Conservative (20 LORAIDs max, robust learning, TESTED)
blocks_per_loraid = 12  

# Balanced (80 LORAIDs max, good learning, TESTED)  
blocks_per_loraid = 3

# Aggressive (120 LORAIDs max, questionable learning capacity)
blocks_per_loraid = 2

# Maximum (240 LORAIDs max, risky for character learning)
blocks_per_loraid = 1
```

### **Supported Argument Formats**
All LORAID arguments support dual formats:
```bash
# Space-separated format
--LORAID 1 2 3 4
--lora_multiplier 1.0 0.5 0.7 1.0

# Repeated flag format  
--LORAID 1 --LORAID 2 --LORAID 3 --LORAID 4
--lora_multiplier 1.0 --lora_multiplier 0.5
```

---

## üîß **Technical Benefits**

### **1. Zero Parameter Conflicts**
```bash
# Validation output confirms no overlaps
INFO: LoRA 0 contains parameters for blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
INFO: LoRA 1 contains parameters for blocks: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23] 
INFO: No block overlaps detected - LoRAs should be independent
```

### **2. Preserved Alpha Values**
Each LoRA maintains its own alpha/scale independently:
```python
# Each LoRA uses its own alpha - no interference
alpha_lora1 = lora1_sd.get("alpha", dim)  # e.g., 64
scale_lora1 = alpha_lora1 / dim           # e.g., 1.0

alpha_lora2 = lora2_sd.get("alpha", dim)  # e.g., 64
scale_lora2 = alpha_lora2 / dim           # e.g., 1.0

# Final weight = base_weight + delta_lora1 + delta_lora2
```

### **3. LoRa rank and parameter space calculations**

üìä Parameter Space Analysis

  Base Model Architecture:

  - 240 blocks total
  - ~10 weight matrices per block (self_attn: q,k,v,o + cross_attn: q,k,v,o + ffn: 2 layers)
  - Each matrix: 5120√ó5120 = 26.2M parameters
  - Parameters per block: ~262M

  LoRA Parameter Calculation:

  For rank r on a 5120√ó5120 matrix:
  - lora_down: 5120 √ó r parameters
  - lora_up: r √ó 5120 parameters
  - Total per matrix: 2 √ó 5120 √ó r = 10,240 √ó r parameters

  üéØ Optimal Rank Calculations

  blocks_per_loraid = 1 (240 LORAIDs max)

  Available space: 1 block √ó 10 matrices = 10 matrices
  Total parameters per matrix: 26.2M
  LoRA parameters per matrix: 10,240 √ó r

  Utilization ratio: (10,240 √ó r) / 26.2M
  For 50% utilization: r = 26.2M √ó 0.5 / 10,240 ‚âà 1,280
  For 25% utilization: r = 26.2M √ó 0.25 / 10,240 ‚âà 640
  For 10% utilization: r = 26.2M √ó 0.1 / 10,240 ‚âà 256

  Recommended rank: 256-640

  blocks_per_loraid = 3 (80 LORAIDs max)

  Available space: 3 blocks √ó 10 matrices = 30 matrices
  Total parameters: 3 √ó 262M = 786M
  LoRA parameters: 30 √ó 10,240 √ó r = 307,200 √ó r

  For 10% utilization: r = 786M √ó 0.1 / 307,200 ‚âà 256
  For 5% utilization: r = 786M √ó 0.05 / 307,200 ‚âà 128

  Recommended rank: 128-256

  blocks_per_loraid = 6 (40 LORAIDs max)

  Available space: 6 blocks √ó 10 matrices = 60 matrices
  Total parameters: 6 √ó 262M = 1.57B
  LoRA parameters: 60 √ó 10,240 √ó r = 614,400 √ó r

  For 5% utilization: r = 1.57B √ó 0.05 / 614,400 ‚âà 128
  For 2.5% utilization: r = 1.57B √ó 0.025 / 614,400 ‚âà 64

  Recommended rank: 64-128

  blocks_per_loraid = 12 (20 LORAIDs max - current)

  Available space: 12 blocks √ó 10 matrices = 120 matrices
  Total parameters: 12 √ó 262M = 3.14B
  LoRA parameters: 120 √ó 10,240 √ó r = 1,228,800 √ó r

  For 2.5% utilization: r = 3.14B √ó 0.025 / 1,228,800 ‚âà 64
  For 1.25% utilization: r = 3.14B √ó 0.0125 / 1,228,800 ‚âà 32

  Current rank 64: Only ~2.5% utilization!
  Recommended rank: 64-128

  üìã Optimization Recommendations

  | Blocks per LORAID | Max LORAIDs | Optimal Rank | Parameter Utilization | File Size (est.) |
  |-------------------|-------------|--------------|-----------------------|------------------|
  | 1                 | 240         | 256-640      | 10-25%                | ~2-5GB           |
  | 3                 | 80          | 128-256      | 5-10%                 | ~1.5-3GB         |
  | 6                 | 40          | 64-128       | 2.5-5%                | ~800MB-1.5GB     |
  | 12                | 20          | 64-128       | 1.25-2.5%             | ~600MB-1.2GB     |

  üéØ Strategic Recommendations

  For Maximum Efficiency:

  - blocks_per_loraid = 3, rank = 128: Best balance of efficiency and scalability
  - 80 LORAIDs max with good parameter utilization

  Current Setup (blocks_per_loraid = 12):

  - Rank 64 is actually reasonable! (~2.5% utilization)
  - Could increase to rank 128 for better utilization (~5%)
  - Going higher than 128 might be overkill for a character

  For Maximum Scale:

  - blocks_per_loraid = 1, rank = 256: 240 characters possible
  - Higher computational cost but maximum character diversity

  üí° Key Insight

  Rank 64 with 12 blocks per LORAID is actually using the parameter space quite reasonably at ~2.5%
  utilization. Going to rank 128 would double utilization to ~5%, which is still very manageable and would give
  more expressive power per character! üéØ

### **4. Deterministic Behavior**
- **LORAID 1** always targets blocks 0-11
- **LORAID 20** always targets blocks 228-239  
- **Reproducible** parameter allocation across training sessions
- **Predictable** inference behavior

### **5. Scalable Architecture**
- Train **240 independent character LoRAs** 
- **Hardware-limited training**: Process large datasets in slices
- **Memory efficiency**: Load only needed LoRAs during inference
- **Deployment flexibility**: Distribute individual LoRAs or merged models

---

## üö® **Important Notes**

### **Neural Network Composition Effects**
When multiple LORAIDs are active simultaneously:
- **Expected behavior**: Signal flows through multiple modified block ranges
- **Compositional output**: Natural blending of learned features through network processing
- **Not a bug**: This is the intended behavior of independent parameter modifications

### **Training Considerations**  
- **Undertrained LoRAs**: May not override base model biases effectively
- **Training duration**: Longer training produces stronger, more distinct features
- **Base model bias**: Strong base model preferences can leak through weak LoRA adaptations

### **Compatibility**
- **Backward compatible**: Works with existing LoRA workflows
- **Metadata preserved**: LORAID information survives EMA and democratic blending
- **Standard tools**: Compatible with existing LoRA utilities and formats

---

## üìä **Performance Characteristics**

### **Memory Usage**
- **Training**: Same as standard LoRA (only target parameters loaded)
- **Inference**: Scales linearly with number of active LORAIDs
- **Merging**: No additional overhead

### **Computational Overhead**
- **Training**: Negligible (parameter filtering happens once)
- **Inference**: Minimal (standard LoRA application per active LORAID)
- **Merging**: Same as standard LoRA merging

### **Storage Requirements**
- **Per LoRA**: ~600-1700MB depending on rank and target blocks
- **Metadata**: <1KB additional per LoRA for LORAID information
- **Merged models**: Same size as standard merged models

---

## üéØ **Use Cases**

### **Character Training Pipeline**
1. **Slice large datasets** by character identity
2. **Train individual LORAIDs** for each character (LORAID 1, 2, 3...)
3. **Validate independence** using single-LORAID inference
4. **Combine as needed** for multi-character scenes
5. **Merge permanently** for production deployment

### **Style Transfer Applications**  
- **Art styles**: Different LORAIDs for different artistic styles
- **Photography modes**: Portrait, landscape, macro each with dedicated LORAIDs
- **Content types**: Architecture, nature, people with separate parameter spaces

### **Fine-Grained Control**
- **Strength adjustment**: Individual multipliers per LORAID
- **Selective activation**: Enable/disable specific LORAIDs per generation
- **A/B testing**: Compare different LORAID combinations systematically

---

## üî¨ **Validation & Testing**

The LORAID system has been validated with:
- ‚úÖ **Parameter space isolation**: No block overlaps detected  
- ‚úÖ **Independent learning**: Single LORAID produces clean results
- ‚úÖ **Multiplier control**: 0.0 multiplier completely disables LORAID
- ‚úÖ **Metadata preservation**: LORAID survives all processing pipelines
- ‚úÖ **Merging functionality**: Permanent weight merging works correctly
- ‚úÖ **Scalability testing**: Supports 240+ theoretical LORAIDs

---

## üöÄ **Getting Started**

1. **Train your first LORAID**:
   ```bash
   --LORAID 1 --output_name character1
   ```

2. **Test single-character inference**:
   ```bash
   --LORAID 1 --lora_multiplier 1.0
   ```

3. **Train additional LORAIDs**:
   ```bash
   --LORAID 2 --output_name character2  
   ```

4. **Test multi-character composition**:
   ```bash
   --LORAID 1 2 --lora_multiplier 1.0 1.0
   ```

5. **Merge for production**:
   ```bash
   --LORAID 1 2 --save_merged_model merged.safetensors
   ```

The LORAID system transforms LoRA training from a conflicted parameter sharing approach to a clean, deterministic parameter space allocation system, enabling scalable character training without catastrophic forgetting.
