# UI Text Dictionary for potential i18n
# I18N Configuration
I18N_DATA = {
    "en": {
        "app_title": "Musubi Tuner GUI",
        "app_header": "# Musubi Tuner GUI",
        "app_desc": "A simple frontend for training LoRA models with Musubi Tuner.",
        "acc_project": "Project Settings",
        "desc_project": "All working files will be created under this directory.",
        "lbl_proj_dir": "Project Working Directory",
        "ph_proj_dir": "Absolute path to your project folder",
        "btn_init_project": "Initialize/Load Project",
        "acc_model": "Model & Dataset Configuration",
        "desc_model": "Choose the model architecture and specify the ComfyUI models directory.",
        "lbl_model_arch": "Model Architecture",
        "lbl_vram": "VRAM Size (GB)",
        "lbl_comfy_dir": "ComfyUI Models Directory",
        "ph_comfy_dir": "Absolute path to ComfyUI/models",
        "btn_validate_models": "Validate Models Directory",
        "header_dataset": "Dataset Settings",
        "desc_dataset": "Configure the resolution and batch size for the dataset. Regenerate the dataset config if you change resolution or batch size.",
        "btn_rec_res_batch": "Set Recommended Resolution & Batch Size",
        "lbl_res_w": "Resolution Width",
        "lbl_res_h": "Resolution Height",
        "lbl_num_repeats": "Num Repeats",
        "lbl_batch_size": "Batch Size",
        "btn_gen_config": "Generate Dataset Config",
        "lbl_toml_preview": "TOML Preview",
        "acc_preprocessing": "Preprocessing",
        "desc_preprocessing": "Pre-calculate latents and text encoder outputs required for training.",
        "btn_set_paths": "Set Default Paths",
        "lbl_vae_path": "VAE Path",
        "ph_vae_path": "Path to VAE model",
        "lbl_te1_path": "Text Encoder 1 Path",
        "ph_te1_path": "Path to Text Encoder 1",
        "lbl_te2_path": "Text Encoder 2 Path",
        "ph_te2_path": "Path to Text Encoder 2 (Optional)",
        "btn_cache_latents": "Cache Latents",
        "btn_cache_text": "Cache Text Encoder Outputs",
        "lbl_cache_log": "Caching Log Output",
        "acc_training": "Training",
        "desc_training_basic": "Configure the training parameters. If you train with the same name again, the previous LoRA will be overwritten.",
        "desc_training_zimage": "Recommended: Use **bf16** for mixed precision. Because the base model has not been released yet, please use `z_image_de_turbo_v1_bf16.safetensors` as the base model.",
        "btn_rec_params": "Set Recommended Parameters",
        "lbl_dit_path": "Base Model / DiT Path",
        "ph_dit_path": "Path to DiT model",
        "lbl_output_name": "Output LoRA Name",
        "header_basic_params": "Basic Parameters",
        "lbl_dim": "LoRA Rank (Dim)",
        "lbl_lr": "Learning Rate",
        "lbl_epochs": "Epochs",
        "lbl_save_every": "Save Every N Epochs",
        "accordion_advanced": "Advanced Parameters",
        "desc_training_detailed": """
Detailed Explanation
- **Learning Rate**: Controls how much the model weights are updated during training. Lower values are safer but slower.
- **Epochs**: One complete pass through the entire training dataset.
- **Save Every N Epochs**: How often to save the model and generate sample images.
- **Discrete Flow Shift**: A parameter specific to flow matching models.
- **Block Swap**: Offloads model blocks to CPU to save VRAM. Higher values save more VRAM but slow down training. Using pinned memory can speed up Block Swap (64GB+ system RAM recommended).
- **Mixed Precision**: fp16 and bf16 are both supported; which is better depends on the model architecture. For bf16, RTX30xx or higher is required.
- **Gradient Checkpointing**: Saves VRAM by recomputing activations during backward pass.
- **FP8**: Further reduces memory usage by using 8-bit floating point arithmetic.
""",
        "lbl_flow_shift": "Discrete Flow Shift",
        "lbl_block_swap": "Block Swap (Z-Image: 0-28, Qwen: 0-58)",
        "lbl_use_pinned_memory_for_block_swap": "Use Pinned Memory for Block Swap",
        "lbl_mixed_precision": "Mixed Precision",
        "lbl_grad_cp": "Gradient Checkpointing",
        "lbl_fp8_scaled": "FP8 Scaled (DiT) - Enables --fp8_base and --fp8_scaled",
        "lbl_fp8_llm": "FP8 LLM/VLM (Text Encoder)",
        "header_sample_images": "Sample Image Generation",
        "lbl_enable_sample": "Generate Sample Images During Training",
        "lbl_sample_every_n": "Generate Sample Every N Epochs",
        "lbl_sample_prompt": "Sample Prompt",
        "ph_sample_prompt": "Prompt for sample generation",
        "lbl_sample_negative_prompt": "Sample Negative Prompt",
        "ph_sample_negative_prompt": "Negative prompt for sample generation",
        "lbl_sample_w": "Sample Width",
        "lbl_sample_h": "Sample Height",
        "accordion_additional": "Additional Options",
        "desc_additional_args": "Enter any additional command line arguments here. They will be appended to the training command.",
        "lbl_additional_args": "Additional Optional Arguments",
        "ph_additional_args": "--arg value --flag",
        "btn_start_training": "Start Training (New Window)",
        "acc_post_processing": "6. Post-Processing",
        "desc_post_proc": "Convert LoRA to ComfyUI format.",
        "lbl_input_lora": "Input LoRA Path",
        "ph_input_lora": "Path to trained .safetensors file",
        "lbl_output_comfy": "Output ComfyUI LoRA Path",
        "ph_output_comfy": "Path to save converted model",
        "btn_convert": "Convert to ComfyUI Format",
        "lbl_conversion_log": "Conversion Log",
        "desc_qwen_notes": "Qwen-Image specific notes here.",

        # New Wizard Tabs
        "tab_init": "1. Initialization",
        "tab_model": "2. Model & Performance",
        "tab_dataset": "3. Dataset Editor",
        "tab_training": "4. Training & Caching",
        "tab_tools": "5. Tools & Preview",

        # Shared & Architecture Specific labels
        "lbl_dit": "DiT Checkpoint",
        "lbl_vae": "VAE Checkpoint",
        "lbl_te1": "Text Encoder 1",
        "lbl_te2": "Text Encoder 2",
        "lbl_img_enc": "Image Encoder (SigLIP/CLIP)",
        "lbl_byt5": "ByT5 Checkpoint",
        "lbl_task": "Task (T2V/I2V)",
        "lbl_fp8_vl": "FP8 Vision-Language",
        "lbl_vae_tiling": "VAE Spatial Tiling",
        "lbl_vae_chunk": "VAE Chunk Size",
        "lbl_vae_sample": "VAE Sample Size",
        "lbl_attn_mode": "Attention Mode",
        "lbl_split_attn": "Split Attention (Save VRAM)",
        "lbl_sage_attn": "Use SageAttention",
        "lbl_flash_attn": "Use FlashAttention",
        "lbl_compile": "Enable torch.compile",
        "lbl_blocks_to_swap": "Blocks to Swap (CPU Offload)",
        "lbl_pinned_mem": "Use Pinned Memory for Swap",
        "lbl_discrete_flow_shift": "Discrete Flow Shift",
        "lbl_timestep_sampling": "Timestep Sampling",
        "lbl_guidance_scale": "Guidance Scale",
        "lbl_embedded_cfg": "Embedded CFG Scale",
        
        # Action buttons
        "btn_cache_latents": "Cache Latents",
        "btn_cache_text": "Cache Text Encoder",
        "btn_copy_cmd": "Copy Command to Clipboard",
        "lbl_cmd_preview": "Command Line Preview",

        # Advanced Training Options
        "header_optimizer": "Optimizer & Learning Rate",
        "lbl_optimizer_type": "Optimizer Type",
        "lbl_optimizer_args": "Optimizer Args (key=val)",
        "lbl_lr_scheduler": "LR Scheduler",
        "lbl_lr_warmup": "Warmup Steps",
        "lbl_lr_decay": "Decay Steps",
        "lbl_lr_num_cycles": "LR Num Cycles",
        "lbl_lr_power": "LR Power",
        "lbl_lr_min_ratio": "Min LR Ratio",

        "header_flow_timestep": "Flow Matching & Timestep Sampling",
        "lbl_weighting_scheme": "Weighting Scheme",
        "lbl_logit_mean": "Logit Mean",
        "lbl_logit_std": "Logit Std",
        "lbl_mode_scale": "Mode Scale",
        "lbl_min_timestep": "Min Timestep",
        "lbl_max_timestep": "Max Timestep",
        "lbl_preserve_dist": "Preserve Distribution Shape",
        "lbl_ts_buckets": "Timestep Buckets",

        "header_network_lora": "Network & LoRA Options",
        "lbl_network_alpha": "Network Alpha",
        "lbl_network_dropout": "Network Dropout",
        "lbl_network_args": "Network Args (key=val)",
        "lbl_dim_from_weights": "Determine Dim from Weights",

        "header_training_flow": "Training Flow & Hardware",
        "lbl_seed": "Random Seed",
        "lbl_max_grad_norm": "Max Gradient Norm",
        "lbl_grad_acc": "Gradient Accumulation Steps",
        "lbl_resume": "Resume from State (Path/HF)",
        "lbl_grad_cp_cpu": "Grad Checkpoint CPU Offload",
        "lbl_offload_inputs": "Offload Inputs to CPU",
        "lbl_numpy_memmap": "Disable Numpy Memmap",

        "header_metadata": "SAI Model Metadata",
        "lbl_meta_title": "Metadata Title",
        "lbl_meta_author": "Metadata Author",
        "lbl_meta_desc": "Metadata Description",
        "lbl_meta_license": "Metadata License",
        "lbl_meta_tags": "Metadata Tags (comma separated)",

        "header_huggingface": "HuggingFace Integration",
        "lbl_hf_repo_id": "HF Repo ID",
        "lbl_hf_repo_type": "HF Repo Type",
        "lbl_hf_path": "HF Path in Repo",
        "lbl_hf_token": "HF Token",
        "lbl_hf_async": "Async Upload",
    },
    "ja": {
        "app_title": "Musubi Tuner GUI",
        "app_header": "# Musubi Tuner GUI",
        "app_desc": "Musubi TunerでLoRAモデルを学習するためのシンプルなフロントエンドです。",
        "acc_project": "1. プロジェクト設定",
        "desc_project": "すべての作業ファイルはこのディレクトリ下に作成されます。",
        "lbl_proj_dir": "プロジェクト作業ディレクトリ",
        "ph_proj_dir": "プロジェクトフォルダへの絶対パス",
        "btn_init_project": "プロジェクトを初期化/読み込み",
        "acc_model": "2. モデル＆データセット設定",
        "desc_model": "モデルアーキテクチャを選択し、ComfyUIのモデルディレクトリを指定してください。",
        "lbl_model_arch": "モデルアーキテクチャ",
        "lbl_vram": "VRAMサイズ (GB)",
        "lbl_comfy_dir": "ComfyUI モデルディレクトリ",
        "ph_comfy_dir": "ComfyUI/models への絶対パス",
        "btn_validate_models": "モデルディレクトリを検証",
        "header_dataset": "3. データセット設定",
        "desc_dataset": "データセットの解像度とバッチサイズを設定してください。解像度やバッチサイズを変えた場合は、データセット設定を再生成してください。",
        "btn_rec_res_batch": "推奨解像度とバッチサイズを設定",
        "lbl_res_w": "解像度 幅",
        "lbl_res_h": "解像度 高さ",
        "lbl_num_repeats": "繰り返し回数 (Num Repeats)",
        "lbl_batch_size": "バッチサイズ",
        "btn_gen_config": "データセット設定(TOML)を生成",
        "lbl_toml_preview": "TOML プレビュー",
        "acc_preprocessing": "4. 前処理 (Preprocessing)",
        "desc_preprocessing": "学習に必要となるLatentsとテキストエンコーダーの出力を事前計算します。",
        "btn_set_paths": "デフォルトパスを設定",
        "lbl_vae_path": "VAE パス",
        "ph_vae_path": "VAEモデルへのパス",
        "lbl_te1_path": "テキストエンコーダー1 パス",
        "ph_te1_path": "テキストエンコーダー1へのパス",
        "lbl_te2_path": "テキストエンコーダー2 パス",
        "ph_te2_path": "テキストエンコーダー2へのパス (オプション)",
        "btn_cache_latents": "Latentsをキャッシュ",
        "btn_cache_text": "テキストエンコーダー出力をキャッシュ",
        "lbl_cache_log": "キャッシュログ出力",
        "acc_training": "5. 学習 (Training)",
        "desc_training_basic": "学習パラメータを設定してください。学習後、同じ名前で学習すると前のLoRAが上書きされます。",
        "desc_training_zimage": "推奨: 混合精度には **bf16** を使用してください。Baseモデルがリリースされていないため、ostris氏の `z_image_de_turbo_v1_bf16.safetensors` を使用してください。",
        "btn_rec_params": "推奨パラメータを設定",
        "lbl_dit_path": "ベースモデル / DiT パス",
        "ph_dit_path": "DiTモデルへのパス",
        "lbl_output_name": "出力 LoRA 名",
        "header_basic_params": "基本パラメータ",
        "lbl_dim": "LoRAランク (Dim)",
        "lbl_lr": "学習率 (Learning Rate)",
        "lbl_epochs": "エポック数 (Epochs)",
        "lbl_save_every": "Nエポックごとに保存",
        "accordion_advanced": "詳細パラメータ",
        "desc_training_detailed": """
詳細説明
- **学習率 (Learning Rate)**: 学習中にモデルの重みをどれくらい更新するかを制御します。低い値の方が安全ですが、学習が遅くなります。
- **エポック数 (Epochs)**: 学習データセット全体を通す回数です。
- **保存頻度 (Save Every N Epochs)**: モデルの保存とサンプル生成を行う頻度です。
- **Discrete Flow Shift**: Flow Matchingモデル特有のパラメータです。
- **Block Swap**: VRAMを節約するためにモデルブロックをCPUにオフロードします。値を大きくするとVRAMを節約できますが、学習が遅くなります。共有メモリを使うとBlock Swapが高速化されます（64GB以上のメインRAMを推奨）。
- **混合精度 (Mixed Precision)**: モデルアーキテクチャによりfp16とbf16のどちらが適しているかは異なります。bf16はRTX30xx以降のGPUが必要です。
- **Gradient Checkpointing**: Backwardパス中にアクティベーションを再計算することでVRAMを節約します。
- **FP8**: 8ビット浮動小数点演算を使用することでメモリ使用量をさらに削減します。
""",
        "lbl_flow_shift": "Discrete Flow Shift",
        "lbl_block_swap": "Block Swap (Z-Image: 0-28, Qwen: 0-58)",
        "lbl_use_pinned_memory_for_block_swap": "Block Swapに共有メモリを使う",
        "lbl_mixed_precision": "混合精度 (Mixed Precision)",
        "lbl_grad_cp": "Gradient Checkpointing",
        "lbl_fp8_scaled": "FP8 Scaled (DiT) - --fp8_base と --fp8_scaled を有効化",
        "lbl_fp8_llm": "FP8 LLM/VLM (テキストエンコーダー)",
        "header_sample_images": "サンプル画像生成",
        "lbl_enable_sample": "学習中にサンプル画像を生成する",
        "lbl_sample_every_n": "Nエポックごとにサンプルを生成",
        "lbl_sample_prompt": "サンプル画像プロンプト",
        "ph_sample_prompt": "サンプル生成用のプロンプト",
        "lbl_sample_negative_prompt": "サンプル画像ネガティブプロンプト",
        "ph_sample_negative_prompt": "サンプル生成用のネガティブプロンプト",
        "lbl_sample_w": "サンプル画像 幅",
        "lbl_sample_h": "サンプル画像 高さ",
        "accordion_additional": "追加オプション",
        "desc_additional_args": "追加のコマンドライン引数を入力してください。これらは学習コマンドに追加されます。",
        "lbl_additional_args": "追加のオプション引数",
        "ph_additional_args": "--arg value --flag",
        "btn_start_training": "学習を開始 (新しいウィンドウが開きます)",
        "acc_post_processing": "6. 後処理 (Post-Processing)",
        "desc_post_proc": "LoRAをComfyUI形式に変換します。",
        "lbl_input_lora": "入力 LoRA パス",
        "ph_input_lora": "学習済み .safetensors ファイルへのパス",
        "lbl_output_comfy": "出力 ComfyUI LoRA パス",
        "ph_output_comfy": "変換後のモデルの保存先パス",
        "btn_convert": "ComfyUI形式に変換",
        "lbl_conversion_log": "変換ログ",
        "desc_qwen_notes": "Qwen-Image 特有の注意点。",

        # New Wizard Tabs
        "tab_init": "1. 初期設定",
        "tab_model": "2. モデル＆パフォーマンス",
        "tab_dataset": "3. データセット編集",
        "tab_training": "4. 前処理＆学習",
        "tab_tools": "5. ツール＆プレビュー",

        # Shared & Architecture Specific labels
        "lbl_dit": "DiT チェックポイント",
        "lbl_vae": "VAE チェックポイント",
        "lbl_te1": "テキストエンコーダー 1",
        "lbl_te2": "テキストエンコーダー 2",
        "lbl_img_enc": "画像エンコーダー (SigLIP/CLIP)",
        "lbl_byt5": "ByT5 チェックポイント",
        "lbl_task": "タスク (T2V/I2V)",
        "lbl_fp8_vl": "FP8 Vision-Language",
        "lbl_vae_tiling": "VAE 空間タイリング",
        "lbl_vae_chunk": "VAE チャンクサイズ",
        "lbl_vae_sample": "VAE サンプルサイズ",
        "lbl_attn_mode": "アテンションモード",
        "lbl_split_attn": "Split Attention (VRAM節約)",
        "lbl_sage_attn": "SageAttentionを使用",
        "lbl_flash_attn": "FlashAttentionを使用",
        "lbl_compile": "torch.compileを有効化",
        "lbl_blocks_to_swap": "ブロックスワップ (CPUオフロード)",
        "lbl_pinned_mem": "スワップに共有メモリを使用",
        "lbl_discrete_flow_shift": "Discrete Flow Shift",
        "lbl_timestep_sampling": "タイムステップサンプリング",
        "lbl_guidance_scale": "Guidance Scale",
        "lbl_embedded_cfg": "Embedded CFG Scale",
        
        # Action buttons
        "btn_cache_latents": "Latentsをキャッシュ",
        "btn_cache_text": "テキストエンコーダーをキャッシュ",
        "btn_copy_cmd": "コマンドをクリップボードにコピー",
        "lbl_cmd_preview": "コマンドラインプレビュー",

        # Advanced Training Options (JA)
        "header_optimizer": "オプティマイザ & 学習率",
        "lbl_optimizer_type": "オプティマイザの種類",
        "lbl_optimizer_args": "オプティマイザ引数 (key=val)",
        "lbl_lr_scheduler": "LR スケジューラ",
        "lbl_lr_warmup": "ウォームアップステップ数",
        "lbl_lr_decay": "減衰ステップ数",
        "lbl_lr_num_cycles": "LR サイクル数",
        "lbl_lr_power": "LR パワー (Polynomial)",
        "lbl_lr_min_ratio": "最小学習率の比率",

        "header_flow_timestep": "Flow Matching & タイムステップサンプリング",
        "lbl_weighting_scheme": "重み付けスキーム (Weighting)",
        "lbl_logit_mean": "Logit 平均",
        "lbl_logit_std": "Logit 標準偏差",
        "lbl_mode_scale": "モードスケール",
        "lbl_min_timestep": "最小タイムステップ",
        "lbl_max_timestep": "最大タイムステップ",
        "lbl_preserve_dist": "分布の形状を維持 (Preserve Distribution)",
        "lbl_ts_buckets": "タイムステップバケット数",

        "header_network_lora": "ネットワーク & LoRA オプション",
        "lbl_network_alpha": "ネットワーク Alpha",
        "lbl_network_dropout": "ネットワーク Dropout",
        "lbl_network_args": "ネットワーク引数 (key=val)",
        "lbl_dim_from_weights": "重みからランク(Dim)を自動決定",

        "header_training_flow": "学習プロセス & ハードウェア",
        "lbl_seed": "乱数シード (Seed)",
        "lbl_max_grad_norm": "最大勾配ノルム",
        "lbl_grad_acc": "勾配累積ステップ数",
        "lbl_resume": "学習再開 (Path/HF)",
        "lbl_grad_cp_cpu": "Gradient CP をCPUオフロード",
        "lbl_offload_inputs": "入力をCPUにオフロード",
        "lbl_numpy_memmap": "Numpy Memmap を無効化",

        "header_metadata": "SAI モデルメタデータ",
        "lbl_meta_title": "タイトル",
        "lbl_meta_author": "作者",
        "lbl_meta_desc": "説明文",
        "lbl_meta_license": "ライセンス",
        "lbl_meta_tags": "タグ (カンマ区切り)",

        "header_huggingface": "HuggingFace 連携",
        "lbl_hf_repo_id": "HF リポジトリ ID",
        "lbl_hf_repo_type": "HF リポジトリ 種類",
        "lbl_hf_path": "HF リポジトリ内パス",
        "lbl_hf_token": "HF トークン",
        "lbl_hf_async": "非同期アップロード",
    },
}
