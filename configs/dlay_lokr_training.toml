# =============================================================================
# DLAY Persona LyCORIS LoKr + AdamW8bit Training Configuration
# Wan2.2-T2V-A14B
# =============================================================================
# Usage:
#   accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
#       src/musubi_tuner/wan_train_network.py --config_file configs/dlay_lokr_training.toml
#
# Command line arguments override values in this file
# =============================================================================

# =============================================================================
# MODEL PATHS
# =============================================================================
[model]
task = "t2v-A14B"
dit = "/root/models/wan22/dit/wan2.2_t2v_low_noise_14B_fp16.safetensors"
dit_high_noise = "/root/models/wan22/dit/wan2.2_t2v_high_noise_14B_fp16.safetensors"
t5 = "/root/models/wan22/text_encoder/models_t5_umt5-xxl-enc-bf16.pth"
vae = "/root/models/wan22/vae/wan_2.1_vae.safetensors"

# =============================================================================
# DATASET
# =============================================================================
[dataset]
dataset_config = "/root/blissful-tuner/configs/dlay_dataset.toml"

# =============================================================================
# NETWORK (LyCORIS LoKr + DoRA) - HIGH QUALITY CONFIG
# =============================================================================
# Optimized for maximum quality without full-rank training
# File size: ~50-100MB (vs ~2MB with default settings)
[network]
network_module = "networks.lycoris"
network_dim = 64
network_alpha = 32
network_args = [
    "algo=lokr",
    "preset=attn-mlp",
    # Larger factorization for more parameters/expressiveness
    "factor=8",
    "use_tucker=True",
    # Enhanced decomposition (both Kronecker matrices get LoRA decomposition)
    "decompose_both=True",
    # DoRA: Weight-Decomposed Low-Rank Adaptation (better fine-tuning quality)
    "dora_wd=True",
    "wd_on_output=True",
    # Learnable scalar with different init strategy
    "use_scalar=True",
    # Regularization for small datasets (269 images) - slightly lower with high dim
    "rank_dropout=0.02",
    # Skip conv layers (Wan2.2 DiT only has 1 Conv3d for patch embedding)
    "conv_dim=0",
    # LoRA+ (increased LR for LoRA-B side)
    "loraplus_lr_ratio=4"
]

# =============================================================================
# OPTIMIZER (AdamW8bit with LoRA+)
# =============================================================================
[optimizer]
optimizer_type = "adamw8bit"
learning_rate = 2e-4
lr_scheduler = "cosine"
lr_warmup_steps = 100

# =============================================================================
# TRAINING
# =============================================================================
[training]
mixed_precision = "fp16"
gradient_accumulation_steps = 2
seed = 42

# Training duration (use steps instead of epochs for easier tracking)
# With 269 images Ã— 3 resolutions, batch=2, grad_accum=2: ~200 steps/epoch
max_train_steps = 5000
save_every_n_steps = 1000

# Timestep settings
timestep_sampling = "shift"
discrete_flow_shift = 3.0
timestep_boundary = 875
num_timestep_buckets = 5

# Memory optimization
# gradient_checkpointing: Trades compute for memory (recomputes activations during backward)
# With B300's 275GB VRAM, you can disable this for faster training
gradient_checkpointing = false

# Mask-weighted loss
use_mask_loss = true
mask_loss_scale = 1.5
mask_min_weight = 0.1

# Regularization
scale_weight_norms = 1.0  # Limit weight norms to prevent overfitting

# Attention - Flash-Attention 2.8.3 (SM103 optimized for B300)
# Options: sdpa (PyTorch SDPA), flash_attn (Flash-Attention 2), xformers, flash3, sage_attn
# Note: cuDNN SDPA is fastest but crashes during training on B300 (backward pass issue)
# flash_attn is stable and uses your custom SM103-compiled Flash-Attention 2.8.3
flash_attn = true
rope_func = "comfy"

# =============================================================================
# SAMPLING
# =============================================================================
[sampling]
sample_prompts = "/root/blissful-tuner/sample_prompts_dlay.txt"
sample_every_n_steps = 1000
sample_at_first = true

# =============================================================================
# OUTPUT
# =============================================================================
[output]
output_dir = "./output/dlay_lokr"
output_name = "dlay_persona_lokr"
logging_dir = "./output/dlay_lokr/logs"
log_prefix = "dlay_lokr_"

# =============================================================================
# ADVANCED OPTIMIZATIONS (Blackwell B300)
# =============================================================================
[advanced]
# Note: torch.compile is disabled due to incompatibility with LyCORIS LoKr dropout
compile = false
compile_mode = "default"
compile_cache_size_limit = 32
cuda_allow_tf32 = true
cuda_cudnn_benchmark = true
