# =============================================================================
# DLAY Persona LoRA + AdamW8bit Training Configuration
# Wan2.2-T2V-A14B
# =============================================================================
# Usage:
#   accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
#       src/musubi_tuner/wan_train_network.py --config_file configs/dlay_lora_training.toml
#
# Command line arguments override values in this file
# =============================================================================

# =============================================================================
# MODEL PATHS
# =============================================================================
[model]
task = "t2v-A14B"
dit = "/root/models/wan22/dit/wan2.2_t2v_low_noise_14B_fp16.safetensors"
dit_high_noise = "/root/models/wan22/dit/wan2.2_t2v_high_noise_14B_fp16.safetensors"
t5 = "/root/models/wan22/text_encoder/models_t5_umt5-xxl-enc-bf16.pth"
vae = "/root/models/wan22/vae/wan_2.1_vae.safetensors"

# =============================================================================
# DATASET
# =============================================================================
[dataset]
dataset_config = "/root/blissful-tuner/configs/dlay_dataset.toml"

# =============================================================================
# NETWORK (Standard LoRA)
# =============================================================================
[network]
network_module = "networks.lora_wan"
network_dim = 128
network_alpha = 64
network_args = [
    "loraplus_lr_ratio=4"
]

# =============================================================================
# OPTIMIZER (AdamW8bit with LoRA+)
# =============================================================================
[optimizer]
optimizer_type = "adamw8bit"
learning_rate = 1.5e-4
lr_scheduler = "cosine"
lr_warmup_steps = 100

# =============================================================================
# TRAINING
# =============================================================================
[training]
mixed_precision = "fp16"
gradient_accumulation_steps = 2
seed = 42

# Training duration (use steps instead of epochs for easier tracking)
# With 269 images Ã— 3 resolutions, batch=2, grad_accum=2: ~200 steps/epoch
max_train_steps = 5000
save_every_n_steps = 1000

# Timestep settings
timestep_sampling = "shift"
discrete_flow_shift = 3.0
timestep_boundary = 875
num_timestep_buckets = 5

# Memory optimization
# gradient_checkpointing: Trades compute for memory (recomputes activations during backward)
# With B300's 275GB VRAM, you can disable this for faster training
gradient_checkpointing = false

# Mask-weighted loss
use_mask_loss = true
mask_min_weight = 0.1

# Regularization
scale_weight_norms = 1.0  # Limit weight norms to prevent overfitting

# Attention - Flash-Attention 2.8.3 (SM103 optimized for B300)
# Options: sdpa (PyTorch SDPA), flash_attn (Flash-Attention 2), xformers, flash3, sage_attn
# Note: cuDNN SDPA is fastest but crashes during training on B300 (backward pass issue)
# flash_attn is stable and uses your custom SM103-compiled Flash-Attention 2.8.3
flash_attn = true
rope_func = "comfy"

# =============================================================================
# SAMPLING
# =============================================================================
[sampling]
sample_prompts = "/root/blissful-tuner/sample_prompts_dlay.txt"
sample_every_n_steps = 1000
sample_at_first = true

# =============================================================================
# OUTPUT
# =============================================================================
[output]
output_dir = "./output/dlay_lora"
output_name = "dlay_persona_lora"
logging_dir = "./output/dlay_lora/logs"
log_prefix = "dlay_lora_"

# =============================================================================
# ADVANCED OPTIMIZATIONS (Blackwell B300)
# =============================================================================
[advanced]
compile = true
compile_mode = "default"
compile_cache_size_limit = 32
cuda_allow_tf32 = true
cuda_cudnn_benchmark = true
